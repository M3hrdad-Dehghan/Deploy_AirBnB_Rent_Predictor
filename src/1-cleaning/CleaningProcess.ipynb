{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is stored as an Excel file. So, we use `Pandas` to load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xb3 in position 10: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m test_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../data/test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m load_data(train_path)\n\u001b[1;32m----> 5\u001b[0m df_test \u001b[38;5;241m=\u001b[39m load_data(test_path)\n",
      "Cell \u001b[1;32mIn[60], line 2\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(file_path):\n\u001b[1;32m----> 2\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Users\\mehrd\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\mehrd\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\mehrd\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\mehrd\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mehrd\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:574\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:663\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xb3 in position 10: invalid start byte"
     ]
    }
   ],
   "source": [
    "train_path = '../../data/train.csv'\n",
    "test_path = '../../data/test.csv'\n",
    "\n",
    "df = load_data(train_path)\n",
    "df_test = load_data(test_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Remove Extra Columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we don't need the `id` , `name` , `host_id` , `host_name` columns, because it does not bring any value for model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['id', 'name', 'host_id', 'host_name' , 'latitude' , 'longitude'], inplace=True)\n",
    "df_test.drop(columns=['id', 'name', 'host_id', 'host_name', 'latitude' , 'longitude'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Columns Typecasting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we can change name of the columns for having better understaing on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Location', 'Area', 'Type', 'Price', 'MinDayNights', 'CountReview',\n",
      "       'LastDateReview', 'AvgReview', 'TotalHostListings', 'DayAvailability'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def change_column_name(df):\n",
    "  new_column_names = {\n",
    "      'neighbourhood_group': 'Location',\n",
    "      'neighbourhood': 'Area',\n",
    "      'room_type': 'Type',\n",
    "      'price': 'Price',\n",
    "      'minimum_nights': 'MinDayNights',\n",
    "      'number_of_reviews': 'CountReview',\n",
    "      'last_review': 'LastDateReview',\n",
    "      'reviews_per_month': 'AvgReview',\n",
    "      'calculated_host_listings_count': 'TotalHostListings',\n",
    "      'availability_365': 'DayAvailability'\n",
    "  }\n",
    "  df = df.rename(columns=new_column_names)\n",
    "  return df\n",
    "\n",
    "df = change_column_name(df)\n",
    "df_test = change_column_name(df_test)\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we should make sure the data type of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location              object\n",
      "Area                  object\n",
      "Type                  object\n",
      "Price                  int64\n",
      "MinDayNights           int64\n",
      "CountReview            int64\n",
      "LastDateReview        object\n",
      "AvgReview            float64\n",
      "TotalHostListings      int64\n",
      "DayAvailability        int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no issue in terms of data type and they are showing correct format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Handle Missing Values**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we should check whether data has any missing value. I create a for loop to iterate among all columns to realize whether they have null values or not. Iâ€™m looking for the number of null values in every single column as well as the percentage of null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location - 0 - %0.0\n",
      "Area - 0 - %0.0\n",
      "Type - 0 - %0.0\n",
      "Price - 0 - %0.0\n",
      "MinDayNights - 0 - %0.0\n",
      "CountReview - 0 - %0.0\n",
      "LastDateReview - 5633 - %20.574\n",
      "AvgReview - 5633 - %20.574\n",
      "TotalHostListings - 0 - %0.0\n",
      "DayAvailability - 0 - %0.0\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    number_null = df.loc[: , col].isnull().sum()\n",
    "    perc_null = (number_null / df.shape[0]) * 100\n",
    "    print('{} - {} - %{}'.format(col, number_null, round(perc_null,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result shows we must have a different approach to handling null values:\n",
    "\n",
    "**Categorical**:\n",
    "* less than 5%, I drop the rows.\n",
    "* between 5% and 30%, I impute with mode. `LastDateReview`\n",
    "* More than 30%, create a new label as Unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LastDateReview'] = df['LastDateReview'].fillna(df['LastDateReview'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical:**\n",
    "* between 0% and 10%, I impute with mean or median.\n",
    "* between 10% to 60%, i impute with KNN. `AvgReview`\n",
    "* More than 60%, I drop the rows.\n",
    "\n",
    "However, the best way is consulting with expert domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df[['AvgReview']] = imputer.fit_transform(df[['AvgReview']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Handle Duplicate Rows**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should handle duplicate rows. Since all values might be same, we just we need to check whether there are two rows that all values in all columns are the same or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame has duplicate rows.\n"
     ]
    }
   ],
   "source": [
    "duplicate_rows = df.duplicated()\n",
    "\n",
    "if duplicate_rows.any():\n",
    "    print(\"The DataFrame has duplicate rows.\")\n",
    "else:\n",
    "    print(\"The DataFrame does not have duplicate rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we remove the duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Numerical Sanity Check**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, I declare all number variables. The number variables are: `Price` , `MinDayNights` , `CountReview` , `AvgReview` , `TotalHostListings` , `DayAvailability`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I must make sure about the data type of the number variable. Just because the column shows numbers, it doesn't mean that they are numbers.Thus, with regular expression I should clean them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no issue with Price\n",
      "There is no issue with MinDayNights\n",
      "There is no issue with CountReview\n",
      "There is no issue with AvgReview\n",
      "There is no issue with TotalHostListings\n",
      "There is no issue with DayAvailability\n"
     ]
    }
   ],
   "source": [
    "def check_non_numeric(df, column_name):\n",
    "    non_numeric_values = df[column_name][~df[column_name].apply(lambda x: isinstance(x, (int, float)))].unique()\n",
    "\n",
    "    if len(non_numeric_values) > 0:\n",
    "        return True, non_numeric_values.tolist()\n",
    "    else:\n",
    "        return \"There is no issue with\"\n",
    "\n",
    "for i in numeric_columns:\n",
    "  print(check_non_numeric(df, i) , i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result shows all data are in correct and logical range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Categorical Sanity Check**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, I declare all categorical variables. These variables are: `Location`, `Area`, `Type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = df.select_dtypes(exclude=['int64', 'float64']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we must make sure about the possible categories for each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cat_Checker(CatIndex):\n",
    "    values = np.sort(df[cat_columns[CatIndex]].unique())\n",
    "    return cat_columns[CatIndex] , values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Location',\n",
       " array(['Bronx', 'Brooklyn', 'Manhattan', 'Queens', 'Staten Island'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cat_Checker(0) #Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Area',\n",
       " array(['Allerton', 'Arden Heights', 'Arrochar', 'Arverne', 'Astoria',\n",
       "        'Bath Beach', 'Battery Park City', 'Bay Ridge', 'Bay Terrace',\n",
       "        'Baychester', 'Bayside', 'Bayswater', 'Bedford-Stuyvesant',\n",
       "        'Belle Harbor', 'Bellerose', 'Belmont', 'Bensonhurst',\n",
       "        'Bergen Beach', 'Boerum Hill', 'Borough Park', 'Breezy Point',\n",
       "        'Briarwood', 'Brighton Beach', 'Bronxdale', 'Brooklyn Heights',\n",
       "        'Brownsville', \"Bull's Head\", 'Bushwick', 'Cambria Heights',\n",
       "        'Canarsie', 'Carroll Gardens', 'Castle Hill', 'Castleton Corners',\n",
       "        'Chelsea', 'Chinatown', 'City Island', 'Civic Center',\n",
       "        'Claremont Village', 'Clason Point', 'Clifton', 'Clinton Hill',\n",
       "        'Co-op City', 'Cobble Hill', 'College Point', 'Columbia St',\n",
       "        'Concord', 'Concourse', 'Concourse Village', 'Coney Island',\n",
       "        'Corona', 'Crown Heights', 'Cypress Hills', 'DUMBO',\n",
       "        'Ditmars Steinway', 'Dongan Hills', 'Douglaston',\n",
       "        'Downtown Brooklyn', 'Dyker Heights', 'East Elmhurst',\n",
       "        'East Flatbush', 'East Harlem', 'East Morrisania', 'East New York',\n",
       "        'East Village', 'Eastchester', 'Edenwald', 'Edgemere', 'Elmhurst',\n",
       "        'Eltingville', 'Emerson Hill', 'Far Rockaway', 'Fieldston',\n",
       "        'Financial District', 'Flatbush', 'Flatiron District', 'Flatlands',\n",
       "        'Flushing', 'Fordham', 'Forest Hills', 'Fort Greene',\n",
       "        'Fort Hamilton', 'Fresh Meadows', 'Glendale', 'Gowanus',\n",
       "        'Gramercy', 'Graniteville', 'Grant City', 'Gravesend',\n",
       "        'Great Kills', 'Greenpoint', 'Greenwich Village', 'Grymes Hill',\n",
       "        'Harlem', \"Hell's Kitchen\", 'Highbridge', 'Hollis', 'Holliswood',\n",
       "        'Howard Beach', 'Howland Hook', 'Huguenot', 'Hunts Point',\n",
       "        'Inwood', 'Jackson Heights', 'Jamaica', 'Jamaica Estates',\n",
       "        'Jamaica Hills', 'Kensington', 'Kew Gardens', 'Kew Gardens Hills',\n",
       "        'Kingsbridge', 'Kips Bay', 'Laurelton', 'Lighthouse Hill',\n",
       "        'Little Italy', 'Little Neck', 'Long Island City', 'Longwood',\n",
       "        'Lower East Side', 'Manhattan Beach', 'Marble Hill',\n",
       "        'Mariners Harbor', 'Maspeth', 'Melrose', 'Middle Village',\n",
       "        'Midland Beach', 'Midtown', 'Midwood', 'Mill Basin',\n",
       "        'Morningside Heights', 'Morris Heights', 'Morris Park',\n",
       "        'Morrisania', 'Mott Haven', 'Mount Eden', 'Mount Hope',\n",
       "        'Murray Hill', 'Navy Yard', 'New Brighton', 'New Dorp',\n",
       "        'New Dorp Beach', 'New Springville', 'NoHo', 'Nolita',\n",
       "        'North Riverdale', 'Norwood', 'Oakwood', 'Olinville', 'Ozone Park',\n",
       "        'Park Slope', 'Parkchester', 'Pelham Bay', 'Pelham Gardens',\n",
       "        'Port Morris', 'Port Richmond', \"Prince's Bay\", 'Prospect Heights',\n",
       "        'Prospect-Lefferts Gardens', 'Queens Village', 'Randall Manor',\n",
       "        'Red Hook', 'Rego Park', 'Richmond Hill', 'Richmondtown',\n",
       "        'Ridgewood', 'Riverdale', 'Rockaway Beach', 'Roosevelt Island',\n",
       "        'Rosebank', 'Rosedale', 'Schuylerville', 'Sea Gate',\n",
       "        'Sheepshead Bay', 'Shore Acres', 'Silver Lake', 'SoHo',\n",
       "        'Soundview', 'South Beach', 'South Ozone Park', 'South Slope',\n",
       "        'Springfield Gardens', 'Spuyten Duyvil', 'St. Albans',\n",
       "        'St. George', 'Stapleton', 'Stuyvesant Town', 'Sunnyside',\n",
       "        'Sunset Park', 'Theater District', 'Throgs Neck', 'Todt Hill',\n",
       "        'Tompkinsville', 'Tottenville', 'Tremont', 'Tribeca',\n",
       "        'Two Bridges', 'Unionport', 'University Heights',\n",
       "        'Upper East Side', 'Upper West Side', 'Van Nest', 'Vinegar Hill',\n",
       "        'Wakefield', 'Washington Heights', 'West Brighton', 'West Village',\n",
       "        'Westchester Square', 'Westerleigh', 'Whitestone',\n",
       "        'Williamsbridge', 'Williamsburg', 'Windsor Terrace', 'Woodhaven',\n",
       "        'Woodlawn', 'Woodrow', 'Woodside'], dtype=object))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cat_Checker(1) #Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Type',\n",
       " array(['Entire home/apt', 'Private room', 'Shared room'], dtype=object))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cat_Checker(2) #Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CHECKPOINT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'df_test' (DataFrame)\n",
      "Stored 'df' (DataFrame)\n",
      "Stored 'numeric_columns' (list)\n",
      "Stored 'cat_columns' (list)\n"
     ]
    }
   ],
   "source": [
    "%store df_test\n",
    "%store df\n",
    "%store numeric_columns\n",
    "%store cat_columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
